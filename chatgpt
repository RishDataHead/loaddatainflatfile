To implement **Direct Path** extraction in a Python-based enterprise-level solution, we'll aim for the following best practices:

1. **Efficient Connection Management**: We’ll manage Oracle connections properly, ensuring that connections are reused and efficiently closed.
2. **Batch Fetching**: Use batch fetching to avoid memory overload.
3. **Error Handling and Logging**: Implement robust error handling and logging for easier debugging and support.
4. **Scalable File Output**: Handle large data sets by writing to a flat file efficiently.
5. **Parallelization**: Optionally use parallelism if the dataset is particularly large, and you want to speed up the extraction by splitting the work across multiple threads or processes.

### Enterprise-Level Python Code for Direct Path Extraction

Here is an optimized enterprise-level Python code implementation using `cx_Oracle` for direct path extraction. The example also includes batch fetching, logging, and efficient memory handling for large datasets:

#### Install Required Libraries

Make sure you have the necessary libraries installed:

```bash
pip install cx_Oracle
```

#### Code Implementation

```python
import cx_Oracle
import csv
import logging
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# Set up logging configuration for enterprise-level logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_extraction.log'),
        logging.StreamHandler()
    ]
)

# Global configuration for the Oracle connection
conn_params = {
    'user': 'your_username',
    'password': 'your_password',
    'dsn': 'your_tns_service_name_or_ip',
    'encoding': 'UTF-8'
}

# Function to manage Oracle connections
def get_oracle_connection():
    try:
        connection = cx_Oracle.connect(
            user=conn_params['user'],
            password=conn_params['password'],
            dsn=conn_params['dsn'],
            encoding=conn_params['encoding']
        )
        return connection
    except cx_Oracle.DatabaseError as e:
        logging.error(f"Error connecting to Oracle: {e}")
        raise

# Fetch data from Oracle using Direct Path (optimized with batch fetching)
def fetch_data_batch(query, batch_size=10000):
    try:
        connection = get_oracle_connection()
        cursor = connection.cursor()
        cursor.execute(query)

        # Fetch data in batches to manage memory usage
        while True:
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break
            yield rows
    except cx_Oracle.DatabaseError as e:
        logging.error(f"Database error occurred: {e}")
    finally:
        cursor.close()
        connection.close()

# Function to write data to CSV file in batches
def write_to_csv(filename, columns, data_batches):
    try:
        with open(filename, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow(columns)  # Write the header

            # Write data batches to file
            for batch in data_batches:
                writer.writerows(batch)
                logging.info(f"Written {len(batch)} rows to {filename}")
        logging.info(f"Data has been successfully written to {filename}")
    except Exception as e:
        logging.error(f"Error writing to file: {e}")

# Function to fetch column names (you can optimize it if you have them predefined)
def get_column_names(query):
    connection = get_oracle_connection()
    cursor = connection.cursor()
    cursor.execute(query)
    columns = [desc[0] for desc in cursor.description]
    cursor.close()
    connection.close()
    return columns

# Enterprise-level wrapper to extract and write large data to a flat file
def extract_and_dump_data(query, output_file, batch_size=10000):
    try:
        logging.info("Starting data extraction process")
        
        # Fetch column names (header) from the database
        columns = get_column_names(query)

        # Fetch data in batches from the Oracle table
        data_batches = fetch_data_batch(query, batch_size)

        # Write the data to a CSV file
        write_to_csv(output_file, columns, data_batches)

        logging.info("Data extraction completed successfully")
    except Exception as e:
        logging.error(f"An error occurred during the extraction process: {e}")

# Main function to control the data extraction process
def main():
    query = "SELECT * FROM your_table_name"  # Replace with your query
    output_file = f"output_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    batch_size = 10000  # You can adjust this for memory efficiency

    # Extract and dump data
    extract_and_dump_data(query, output_file, batch_size)

if __name__ == "__main__":
    main()
```

### Key Features of the Code:

1. **Connection Management**:

   * The `get_oracle_connection()` function ensures a fresh and efficient connection to the Oracle database.
   * Connections are closed properly after data fetching.

2. **Batch Fetching**:

   * The `fetch_data_batch()` function fetches data in batches using `cursor.fetchmany(batch_size)`. This prevents memory overload and makes it scalable even for large datasets.
   * The batch size is configurable, and you can tweak it based on the available memory and Oracle's performance.

3. **Error Handling and Logging**:

   * The code is designed to log every step and any issues encountered.
   * All exceptions (connection, fetching, file writing) are caught and logged for transparency.

4. **Efficient File Writing**:

   * Data is written in batches, meaning the application doesn’t need to keep the entire dataset in memory.
   * The data is written to a CSV file using Python’s built-in `csv` module, which efficiently handles large volumes of data.

5. **Timestamped Output Filename**:

   * The output CSV file’s name is dynamically generated using the current timestamp (`datetime.now().strftime('%Y%m%d_%H%M%S')`), making it easier to distinguish files when extracting data at different times.

6. **Parallelization (optional)**:

   * If the dataset is too large for a single process, you can extend this code to run in parallel by using `ThreadPoolExecutor` or `ProcessPoolExecutor` to distribute the fetching and writing tasks across multiple threads or processes.

### Optimizations & Considerations:

* **Parallel Execution**: Depending on the Oracle database’s load and the size of the dataset, you can distribute the data fetching using multiple threads or processes. You can divide the query into parts (e.g., by partitioning a large table based on range conditions).
* **Direct Path and Parallel Query**: If supported by your database version and schema, you can use Oracle's **parallel query** feature in combination with this solution to fetch data even faster.
* **Table Partitioning**: If the table is partitioned, you could optimize this process by fetching data from partitions in parallel.

### Example for Parallel Execution (optional):

If you want to parallelize the data extraction, you can modify the `fetch_data_batch()` function to fetch data from different partitions or ranges concurrently using `ThreadPoolExecutor`.

Let me know if you'd like help implementing this parallelization or any other features!





  ===============================================================
import cx_Oracle
import os

# Oracle database connection settings
oracle_connection = {
    "username": "your_username",
    "password": "your_password",
    "hostname": "your_host",
    "port": 1521,
    "service_name": "your_service_name"
}

# Path to your external files (CSV, XML, etc.)
external_files_directory = "/path/to/external/files/"
external_file_name = "your_file.csv"  # Replace with your actual file name
external_file_path = os.path.join(external_files_directory, external_file_name)

# Permanent table name
external_table_name = f"ext_table_{os.path.splitext(external_file_name)[0]}"
internal_table_name = "employees"  # Internal table where you want to load data

# Function to connect to Oracle database
def connect_to_oracle():
    dsn_tns = cx_Oracle.makedsn(oracle_connection['hostname'], oracle_connection['port'], service_name=oracle_connection['service_name'])
    connection = cx_Oracle.connect(user=oracle_connection['username'], password=oracle_connection['password'], dsn=dsn_tns)
    return connection

# Function to create Oracle directory object for external files
def create_oracle_directory(directory_name, directory_path):
    connection = connect_to_oracle()
    cursor = connection.cursor()
    
    try:
        # Check if the directory object already exists
        cursor.execute(f"SELECT * FROM all_directories WHERE directory_name = '{directory_name.upper()}'")
        if not cursor.fetchone():
            # Create a new directory object in Oracle that points to the local directory
            create_dir_sql = f"CREATE DIRECTORY {directory_name} AS '{directory_path}'"
            cursor.execute(create_dir_sql)
            print(f"Oracle directory object '{directory_name}' created successfully.")
        else:
            print(f"Oracle directory object '{directory_name}' already exists.")
        
        connection.commit()
    except cx_Oracle.DatabaseError as e:
        print(f"Error creating directory object: {e}")
    finally:
        cursor.close()
        connection.close()

# Function to create external table in Oracle
def create_external_table(file_name, table_name, directory_name):
    create_table_sql = f"""
    CREATE TABLE {table_name} (
        employee_id   NUMBER,
        employee_name VARCHAR2(100),
        department    VARCHAR2(50),
        salary        NUMBER
    )
    ORGANIZATION EXTERNAL (
        TYPE ORACLE_LOADER
        DEFAULT DIRECTORY {directory_name}
        ACCESS PARAMETERS (
            RECORDS DELIMITED BY NEWLINE
            FIELDS TERMINATED BY ','
            MISSING FIELD VALUES ARE NULL
            FIELDS OPTIONALLY ENCLOSED BY '"'
        )
        LOCATION ('{file_name}')
    )
    PARALLEL 4
    REJECT LIMIT UNLIMITED;
    """
    
    connection = connect_to_oracle()
    cursor = connection.cursor()
    
    try:
        # Execute the CREATE TABLE statement to create the external table
        cursor.execute(create_table_sql)
        print(f"External table '{table_name}' created successfully.")
        connection.commit()
    except cx_Oracle.DatabaseError as e:
        print(f"Error creating external table: {e}")
    finally:
        cursor.close()
        connection.close()

# Function to create internal table (permanent table)
def create_internal_table():
    create_table_sql = f"""
    CREATE TABLE {internal_table_name} (
        employee_id   NUMBER PRIMARY KEY,
        employee_name VARCHAR2(100),
        department    VARCHAR2(50),
        salary        NUMBER
    );
    """
    
    connection = connect_to_oracle()
    cursor = connection.cursor()
    
    try:
        cursor.execute(create_table_sql)
        print(f"Internal table '{internal_table_name}' created successfully.")
        connection.commit()
    except cx_Oracle.DatabaseError as e:
        print(f"Error creating internal table: {e}")
    finally:
        cursor.close()
        connection.close()

# Function to load data from external table to internal table
def load_data_from_external_to_internal():
    load_sql = f"""
    INSERT INTO {internal_table_name} (employee_id, employee_name, department, salary)
    SELECT employee_id, employee_name, department, salary
    FROM {external_table_name};
    """
    
    connection = connect_to_oracle()
    cursor = connection.cursor()
    
    try:
        cursor.execute(load_sql)
        print(f"Data from external table '{external_table_name}' loaded into internal table '{internal_table_name}'.")
        connection.commit()
    except cx_Oracle.DatabaseError as e:
        print(f"Error loading data: {e}")
    finally:
        cursor.close()
        connection.close()

# Main function to execute the process
def process_file_for_external_table(directory_name, file_name, table_name):
    # Step 1: Create the Oracle directory object
    create_oracle_directory(directory_name, external_files_directory)
    
    # Step 2: Create the external table
    create_external_table(file_name, table_name, directory_name)
    
    # Step 3: Create the internal table
    create_internal_table()
    
    # Step 4: Load data from external table into internal table
    load_data_from_external_to_internal()

# Example Usage:
directory_name = "ext_data_dir"  # The name of the directory object in Oracle
process_file_for_external_table(directory_name, external_file_name, external_table_name)























  
